{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bd9750f",
   "metadata": {},
   "source": [
    "# Práctica 1: KNN y selección de atributos\n",
    "Aprendizaje Automático II, 2025-2026\n",
    "\n",
    "**Integrantes:** Nombre Apellido1 Apellido2 (reemplazar por los nombres del grupo)\n",
    "\n",
    "Este notebook contiene la resolución de los ejercicios de la Práctica 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c2e1fe",
   "metadata": {},
   "source": [
    "## 1. Implementación y uso de KNN\n",
    "En esta sección se implementa y utiliza el algoritmo KNN sobre el dataset de cáncer de mama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fa2f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X: (569, 30), Shape y: (569,)\n"
     ]
    }
   ],
   "source": [
    "# a) Descarga de datos\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46e4e230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (398, 30), Test shape: (171, 30)\n"
     ]
    }
   ],
   "source": [
    "# b) Preprocesado: separación, partición y normalización\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Separación ya realizada arriba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "print(f'Train shape: {X_train.shape}, Test shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75cb2fd",
   "metadata": {},
   "source": [
    "### c) Pregunta teórica: orden de normalización y completado de datos ausentes\n",
    "**Respuesta:** Si hay datos ausentes, primero se deben completar (imputar) y luego normalizar. Si se normaliza antes de completar, los valores imputados pueden distorsionar la escala. Si se normaliza antes de dividir en train/test, se produce filtrado de información (data leakage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5251f72c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "KNNClassifier.__init__() got an unexpected keyword argument 'n_neighbors'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34meuclidean_distance\u001b[39m(x1, x2):\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.linalg.norm(x1 - x2)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m knn = \u001b[43mKNNClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistance_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43meuclidean_distance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m knn.fit(X_train, y_train)\n\u001b[32m     10\u001b[39m y_pred_custom = knn.predict(X_test)\n",
      "\u001b[31mTypeError\u001b[39m: KNNClassifier.__init__() got an unexpected keyword argument 'n_neighbors'"
     ]
    }
   ],
   "source": [
    "# d) Importar e instanciar el clasificador KNN personalizado\n",
    "from KNNClassifier import KNNClassifier\n",
    "import numpy as np\n",
    "\n",
    "def euclidean_distance(x1, x2):\n",
    "    return np.linalg.norm(x1 - x2)\n",
    "\n",
    "knn = KNNClassifier(n_neighbors=5, distance_func=euclidean_distance)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_custom = knn.predict(X_test)\n",
    "print('Predicciones (primeros 10):', y_pred_custom[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6dbdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# e) Evaluación del modelo KNN personalizado\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc_custom = accuracy_score(y_test, y_pred_custom)\n",
    "print(f'Accuracy KNN personalizado: {acc_custom:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd80d326",
   "metadata": {},
   "source": [
    "### f) Pregunta teórica: desbalanceo de clases\n",
    "**Respuesta:** Si hay desbalanceo de clases, KNN puede estar sesgado hacia la clase mayoritaria. Una solución es ponderar el voto de los vecinos por la inversa de la distancia o usar técnicas de balanceo como sobremuestreo o submuestreo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7c57c6",
   "metadata": {},
   "source": [
    "### g) Pregunta teórica: coste en memoria de KNN\n",
    "**Respuesta:** KNN almacena todo el conjunto de entrenamiento, por lo que el coste en memoria es O(n·d), siendo n el número de muestras y d el número de atributos. Se puede reducir usando técnicas de reducción de datos como prototipos o clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992c357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# h) Comparación con KNeighborsClassifier de sklearn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_sk = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "knn_sk.fit(X_train, y_train)\n",
    "y_pred_sk = knn_sk.predict(X_test)\n",
    "acc_sk = accuracy_score(y_test, y_pred_sk)\n",
    "print(f'Accuracy sklearn KNN: {acc_sk:.4f}')\n",
    "error_medio = np.mean(y_pred_sk != y_pred_custom)\n",
    "print(f'Error medio entre predicciones: {error_medio:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06c1ead",
   "metadata": {},
   "source": [
    "### i) Pregunta teórica: método alternativo ponderado\n",
    "**Respuesta:** El método ponderado por la inversa de la distancia puede ser mejor si los vecinos más cercanos son más relevantes, pero es más sensible a outliers. KNN estándar da igual peso a los k vecinos. La elección depende del problema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b730785",
   "metadata": {},
   "source": [
    "## 2. Optimización de KNN\n",
    "En esta sección se optimiza el número de vecinos y se exploran diferentes métricas de distancia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c336a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Validación cruzada para encontrar k óptimo\n",
    "from sklearn.model_selection import cross_val_score\n",
    "k_range = range(1, 21)\n",
    "cv_scores = []\n",
    "for k in k_range:\n",
    "    knn_cv = KNeighborsClassifier(n_neighbors=k, metric='euclidean')\n",
    "    scores = cross_val_score(knn_cv, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "k_opt = k_range[np.argmax(cv_scores)]\n",
    "print(f'k óptimo: {k_opt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922d45ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) Accuracy en test con k óptimo\n",
    "knn_opt = KNeighborsClassifier(n_neighbors=k_opt, metric='euclidean')\n",
    "knn_opt.fit(X_train, y_train)\n",
    "y_pred_opt = knn_opt.predict(X_test)\n",
    "acc_opt = accuracy_score(y_test, y_pred_opt)\n",
    "print(f'Accuracy test con k óptimo: {acc_opt:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89705979",
   "metadata": {},
   "source": [
    "### c) Pregunta teórica: ¿k óptimo en test?\n",
    "**Respuesta:** El k óptimo encontrado por validación cruzada maximiza la accuracy en training, pero no necesariamente en test, aunque suele ser un buen estimador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b58b77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d) Gráfica accuracy vs número de vecinos\n",
    "import matplotlib.pyplot as plt\n",
    "acc_train = []\n",
    "acc_test = []\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, metric='euclidean')\n",
    "    knn.fit(X_train, y_train)\n",
    "    acc_train.append(knn.score(X_train, y_train))\n",
    "    acc_test.append(knn.score(X_test, y_test))\n",
    "plt.plot(k_range, acc_train, label='Train')\n",
    "plt.plot(k_range, acc_test, label='Test')\n",
    "plt.xlabel('Número de vecinos (k)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy vs Número de vecinos')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d1a0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# e) Experimentos con distancia de Minkowski para p=1,2,10\n",
    "for p in [1, 2, 10]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k_opt, metric='minkowski', p=p)\n",
    "    knn.fit(X_train, y_train)\n",
    "    acc = knn.score(X_test, y_test)\n",
    "    print(f'Accuracy test con p={p}: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e68e4e7",
   "metadata": {},
   "source": [
    "### f) Pregunta teórica: efecto de p en Minkowski\n",
    "**Respuesta:** p=1 corresponde a la distancia Manhattan, p=2 a Euclídea, p=10 se aproxima a la distancia Chebyshev. El valor óptimo depende de la estructura de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829146ab",
   "metadata": {},
   "source": [
    "## 3. Selección de atributos\n",
    "En esta sección se exploran métodos de reducción de dimensionalidad y su impacto en KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc6e544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) VarianceThreshold\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "u = 0.1\n",
    "vt = VarianceThreshold(threshold=u)\n",
    "X_train_vt = vt.fit_transform(X_train)\n",
    "X_test_vt = vt.transform(X_test)\n",
    "print(f'Número de atributos tras VarianceThreshold: {X_train_vt.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538ae6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) Gráfica accuracy vs umbral u\n",
    "us = np.linspace(0, 1, 20)\n",
    "accs = []\n",
    "for u in us:\n",
    "    vt = VarianceThreshold(threshold=u)\n",
    "    X_train_vt = vt.fit_transform(X_train)\n",
    "    X_test_vt = vt.transform(X_test)\n",
    "    knn = KNeighborsClassifier(n_neighbors=k_opt)\n",
    "    knn.fit(X_train_vt, y_train)\n",
    "    accs.append(knn.score(X_test_vt, y_test))\n",
    "plt.plot(us, accs)\n",
    "plt.xlabel('Umbral de varianza (u)')\n",
    "plt.ylabel('Accuracy en test')\n",
    "plt.title('Accuracy vs Umbral de varianza')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed8f76d",
   "metadata": {},
   "source": [
    "### c) Pregunta teórica: sentido de u=0 y u=1\n",
    "**Respuesta:** u=0 no elimina ningún atributo (salvo los constantes). u=1 elimina todos los atributos salvo los que tengan varianza máxima, lo que normalmente deja muy pocos o ninguno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e99e71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d) SelectKBest\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "K = 10\n",
    "selector = SelectKBest(score_func=f_classif, k=K)\n",
    "X_train_kbest = selector.fit_transform(X_train, y_train)\n",
    "X_test_kbest = selector.transform(X_test)\n",
    "print(f'Número de atributos seleccionados: {X_train_kbest.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0e3b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# e) Búsqueda del mejor K (número de atributos)\n",
    "Ks = range(1, X_train.shape[1]+1)\n",
    "accs_k = []\n",
    "for K in Ks:\n",
    "        selector = SelectKBest(score_func=f_classif, k=K)\n",
    "        X_train_kbest = selector.fit_transform(X_train, y_train)\n",
    "        X_test_kbest = selector.transform(X_test)\n",
    "        knn = KNeighborsClassifier(n_neighbors=k_opt)\n",
    "        knn.fit(X_train_kbest, y_train)\n",
    "        accs_k.append(knn.score(X_test_kbest, y_test))\n",
    "plt.plot(Ks, accs_k)\n",
    "plt.xlabel('Número de atributos (K)')\n",
    "plt.ylabel('Accuracy en test')\n",
    "plt.title('Accuracy vs Número de atributos (SelectKBest)')\n",
    "plt.show()\n",
    "K_best = Ks[np.argmax(accs_k)]\n",
    "        print(f'Mejor K: {K_best}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef648f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f) mRMR (a completar en mRMR.py)\n",
    "from mRMR import mRMR_selection\n",
    "# Suponiendo que mRMR_selection(X, y, K) devuelve los índices de los K mejores atributos\n",
    "K = 10\n",
    "idx_mrmr = mRMR_selection(X_train, y_train, K)\n",
    "X_train_mrmr = X_train[:, idx_mrmr]\n",
    "X_test_mrmr = X_test[:, idx_mrmr]\n",
    "knn = KNeighborsClassifier(n_neighbors=k_opt)\n",
    "knn.fit(X_train_mrmr, y_train)\n",
    "acc_mrmr = knn.score(X_test_mrmr, y_test)\n",
    "print(f'Accuracy con mRMR y K={K}: {acc_mrmr:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf490e8b",
   "metadata": {},
   "source": [
    "### g) Pregunta teórica: mejor valor de k\n",
    "**Respuesta:** El mejor valor de k depende del conjunto de datos y debe determinarse empíricamente mediante validación cruzada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298a7412",
   "metadata": {},
   "source": [
    "### h) Pregunta teórica: papel de la información mutua en mRMR\n",
    "**Respuesta:** La información mutua mide la dependencia entre variables. En mRMR se usa para seleccionar atributos relevantes y no redundantes. Se podría sustituir por otras métricas de dependencia, como la correlación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7c2e8f",
   "metadata": {},
   "source": [
    "### i) Pregunta teórica: comparación de métodos de selección\n",
    "**Respuesta:** mRMR suele ser mejor que métodos univariados como SelectKBest porque considera la redundancia entre atributos, pero es más costoso computacionalmente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
